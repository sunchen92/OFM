..	-*- mode: rst-mode -*-
..
.. Note: This file includes further autogenerated ones.
..
.. Version number is filled in automatically.
.. |version| replace:: 1.1

==========
BroControl
==========

.. rst-class:: opening

    This document summarizes installation and use of *BroControl*,
    Bro's interactive shell for operating Bro installations. *BroControl*
    has two modes of operation: a *stand-alone* mode for
    managing a traditional, single-system Bro setup; and a *cluster*
    mode for maintaining a multi-system setup of coordinated Bro
    instances load-balancing the work across a set of independent
    machines. Below, we describe the installation process separately
    for the two modes. Once installed, the operation is pretty similar
    for both types; just keep in mind that if this document refers to
    "nodes" and you're in a stand-alone setup, there is only a
    single one and no worker/proxies.

.. contents::

Download
--------

You can find the latest BroControl release for download at
http://www.bro-ids.org/download.

BroControl's git repository is located at
`git://git.bro-ids.org/broctl <git://git.bro-ids.org/broctl>`_. You
can browse the repository `here <http://git.bro-ids.org/broctl>`_.

This document describes BroControl |version|. See the ``CHANGES``
file for version history.

Prerequisites
-------------

Running *BroControl* requires the following prerequisites:

  - A Unix system. FreeBSD, Linux, and MacOS are supported and
    should work out of the box. Other Unix systems will quite likely
    require some tweaking. Note that in a cluster setup, all systems
    must be running exactly the *same* operating system.

  - A version of *Python* >= 2.6.

  - A *bash* (note in particular, that on FreeBSD, *bash* is not
    installed by default).

Installation
------------

Stand-alone Bro
~~~~~~~~~~~~~~~

For installing a standalone Bro setup, just follow the Bro quickstart
guide included with the distribution in ``doc/quickstart.rst``.

Bro Cluster
~~~~~~~~~~~

A *Bro Cluster* is a set of systems jointly analyzing the traffic of
a network link in a coordinated fashion. *BroControl* is able to
operate such a setup from a central manager system pretty much
transparently, hiding much of the complexity of the multi-machine
installation.

A cluster consists of four types of components:

  Frontends.
     One or more frontends: Frontends load-balance the traffic
     across a set of worker machines.

  Worker nodes.
       Workers are doing the actual analysis, with each seeing a
       slice of the overall traffic as split by the frontend(s).

  One or more proxies.
       Proxies relay the communication between worker nodes.

  One manager.
       The manager provides the cluster's user-interface for
       controlling and logging. During operation, the user only
       interacts with the manager; this is where *BroControl* is
       running.

For more information about the cluster architecture, including options
for the frontend, see Bro's cluster documentation included with the
distribution in ``doc/cluster.rst``.  This document focuses on the
installation of manager, workers, and the proxies. If not otherwise
stated, in the following we use the terms "manager", "worker", and
"proxy" to refer to Bro instances, not to physical machines; rather,
we use the term "node" to refer to physical machines. There may be
multiple Bro instances running on the same node. For example, it's
possible to run a proxy on the same node as the manager is operating
on.

In the following, as an example setup, we will assume that our
cluster consists of four nodes (not counting the frontend). The host
names of the systems will be ``host1``, ``host2``, ``host3``, and
``host4``. We will configure the cluster so that ``host1`` runs the
manager and the (only) proxy, and ``host{2,3,4}`` are each running
one worker. This is a typical setup, which will work well for many
sites.

When installing a cluster, in addition to the prerequisites
mentioned above, you need to

  - have the same user account set up on all nodes. On the worker
    nodes, this user must have access to target network interface in
    promiscuous mode. ``ssh`` access from the manager node to this
    user account must be setup on all machines, and must work
    without asking for a password/passphrase.

  - have some storage available on all nodes under the same path,
    which we will call the cluster's *prefix* path. In the
    following, we will use ``/usr/local/bro`` as an example. The Bro
    user must be able to either create this directory or, where it
    already exists, must have write permission inside this directory
    on all nodes.

  - have ``ssh`` and ``rsync`` installed.


With all prerequisites in place, perform the following steps to
install a Bro cluster (as the Bro user) if you install from the Bro source
code (which includes BroControl):

- Configure and compile the Bro distribution using the cluster's
  prefix path as ``--prefix``::

  > cd /path/to/bro/source/distribution
  > ./configure --prefix=/usr/local/bro && make && make install

- Add ``<prefix>/bin`` to your ``PATH``.

- Create a cluster configuration file. There is an example provided,
  which you can edit according to the instructions in the file::

  > cd /usr/local/bro
  > vi etc/broctl.cfg

- Create a node configuration file to define where manager, workers,
  and proxies are to run. There is again an example, which defines
  the example scenario described above and can be edited as needed::

  > cd /usr/local/bro
  > vi etc/node.cfg

- Create a network configuration file that lists all of the networks
  which the cluster should consider as local to the monitored
  environment. Once again, the installation installs a template for
  editing::

    > cd /usr/local/bro
    > vi etc/networks.cfg

- Install workers and proxies using *BroControl*::

    > broctl install

  This installation process uses ``ssh`` and ``rdist`` to copy the
  configuration over to the remote machines so, as described above,
  you need to ensure that logging in via SSH works before the install will
  succeed.

- Some tasks need to be run on a regular basis. On the manager node,
  insert a line like this into the crontab of the user running the
  cluster::

      0-59/5 * * * * <prefix>/bin/broctl cron

- Finally, you can start the cluster::

  > broctl start

Getting Started
---------------

*BroControl* is an interactive interface to the cluster which allows
you to, e.g., start/stop the monitoring or update its configuration.
It is started with the ``broctl`` script and then expects commands
on its command-line (alternatively, ``broctl`` can also be started
with a single command directly on the shell's command line)::

  > broctl
  Welcome to BroControl x.y

  Type "help" for help.

  [BroControl] >

As the message says, type help_ to see a list of
all commands. We will now briefly summarize the most important
commands. A full reference follows `Command Reference`_.

Once ``broctl.cfg`` and ``node.cfg`` are set up as described above,
the monitoring can be started with the start_ command. In the cluster
setup, this will successively start manager, proxies, and workers. The
status_ command should then show all nodes as operating. To stop the
monitoring, issue the stop_ command. exit_ leaves the shell.

On the manager system (and on the standalone system), you find the
current set of (aggregated) logs in ``logs/current`` (which is a
symlink to the corresponding spool directory). The workers and proxies
log into ``spool/proxy/`` and ``spool/<worker-name>/``, respectively.
The manager/standalone logs are archived in ``logs/``, by default
once a day. Log files of workers and proxies are discarded at the
same rotation interval.

Whenever the *BroControl* configuration is modified in any way
(including changes to configuration files and site-specific policy
scripts), install_ installs the new version. *No changes will take
effect until* install_ *is run*. Before you run install_, check_ can be
used to check for any potential errors in the new configuration, e.g.,
typos in scripts. If check_ does not report any problems, doing
install_ will pretty likely not break anything.

Note that generally configuration changes only take effect after a
restart of the affected nodes. The restart_ command triggers this.
Some changes however can be put into effect on-the-fly without
restarting any of the nodes by using the update_ command (again only
after doing install_ first). Such dynamic updates generally work with
all changes done which only modify const variables declared as
*redefinable* (i.e., with Bro's *&redef* attribute).

Generally, site-specific tuning needs to be done with local policy
scripts, as in any Bro setup. This is described in
`Site-specific Customization`_.

*BroControl* provides various options to control the behavior of
the setup. These options can be set by editing ``etc/broctl.cfg``.
The config_ command gives a list of all options
with their current values. A list of the most important options also
follows `Option Reference`_.

Site-specific Customization
---------------------------

You'll most likely want to adapt the Bro policy to the local
environment and much of the more specific tuning requires writing
local policy files.

During the initial install, sample local policy scripts (which you can edit)
are installed in ``share/bro/site``. In the stand-alone setup, a single
file called ``local.bro`` gets loaded automatically.  In the cluster
setup, the same ``local.bro`` gets loaded, followed by one of three
other files: ``local-manager.bro``, ``local-worker.bro``, and
``local-proxy.bro`` are loaded by the manager, workers, and proxy,
respectively.

In the cluster setup, the main exception to putting everything into
``local.bro`` is notice filtering, which should be done only on the
manager.

The next scripts that are loaded are the ones that are automatically
generated by BroControl.  These scripts are created from the
``etc/networks.cfg`` and ``etc/broctl.cfg`` files.

The last scripts loaded are any node-specific scripts specified with the
option ``aux_scripts`` in ``etc/node.cfg``.  This option can be used to
load additional scripts to individual nodes only.  For example, one could
add a script ``experimental.bro`` to a single worker for trying out new
experimental code.

The scripts_ command shows precisely which policy scripts get loaded (and
in what order) by a node; that can be very helpful.

If you want to change which local policy scripts are loaded by the nodes,
you can set SitePolicyStandalone_ for all Bro instances,
SitePolicyManager_ for the manager, and SitePolicyWorker_ for the
workers.  To change the directory where local policy scripts are
located, set the option SitePolicyPath_ to a different path.  These
options can be changed in the ``etc/broctl.cfg`` file.

Command Reference
-----------------

The following summary lists all commands supported by *BroControl*.
All commands may be either entered interactively or specified on the
shell's command line. If not specified otherwise, commands taking
*[<nodes>]* as arguments apply their action either to the given set of
nodes, or to all nodes if none is given.

.. include:: commands.rst

Option Reference
----------------

This section summarizes the options that can be set in
``etc/broctl.cfg`` for customizing the behaviour of *BroControl*. This
section summarizes the options that can be set in ``etc/broctl.cfg``
for customizing the behavior of *BroControl*. Usually, one only needs
to change the "user options", which are listed first. The "internal
options" are, as the name suggests, primarily used internally and set
automatically. They are documented here only for reference.

.. include:: options.rst

Writing Plugins
---------------

BroControl provides a plugin interface to extend its functionality. A
plugin is written in Python and can do any, or all, of the following:

    * Perform actions before or after any of the standard BroControl
      commands is executed. When running before the actual command, it
      can filter which nodes to operate or stop the execution
      altogether. When running after the command, it gets access to
      the commands success on a per-node basis (where applicable).

    * Add custom commands to BroControl.

    * Add custom options to BroControl defined in ``broctl.cfg``.

    * Add custom keys to nodes defined in ``node.cfg``.

    * Add custom entries to the ``analysis`` command.

A plugin is written by deriving a new class from BroControl class
`Plugin`_. The Python script with the new plugin is then copied into a
plugin directory searched by BroControl at startup. By default,
BroControl searches ``<prefix>/lib/broctl/plugins``; further may be
configured by setting the PluginDir_ option. Note that any plugin
script must end in ``*.py`` to be found. BroControl comes with two
example plugins that can be used as a starting point; see
``<prefix>/lib/broctl/plugins/*.sample``

In the following, we document the API that is available to plugins. A
plugin must be derived from the `Plugin`_ class, and can use its
methods as well as those of the `Node`_ class.

.. include:: plugins.rst

Miscellaneous
-------------

Mails
~~~~~

*BroControl* sends four types of mails to the address given in
``MailTo``:

1. When logs are rotated (per default once a day), a list of all
   alarms during the last rotation interval is sent. This can be
   disabled by setting ``MailAlarms=0``.

2. When the ``cron`` command notices that a node has crashed, it
   restarts it and sends a notification. It may also send a more
   detailed crash report containing information about the crash.

3. NOTICES with a notice action ``EMAIL``.

4. If `trace-summary <http://www.bro-ids.org/documentation/components/trace-summary/README.html>`_
   is installed, a traffic summary is sent each rotation interval.

Performance Analysis
~~~~~~~~~~~~~~~~~~~~

*TODO*: ``broctl cron`` logs a number of statistics, which can be
analyzed/plotted for understanding the clusters run-time behavior.

Questions and Answers
---------------------

*Can I use an NFS-mounted partition as the cluster's base directory to avoid the ``rsync``'ing?*
    Yes. BroBase_ can be on an NFS partition.
    Configure and install the shell as usual with
    ``--prefix=<BroBase>``. Then add ``HaveNFS=1`` and
    ``SpoolDir=<spath>`` to ``etc/broctl.cfg``, where ``<spath>`` is a
    path on the local disks of the nodes; ``<spath>`` will be used for
    all non-shared data (make sure that the parent directory exists
    and is writable on all nodes!). Then run ``make install`` again.
    Finally, you can remove ``<BroBase>/spool`` (or link it to <spath>).
    In addition, you might want to keep the log files locally on the nodes
    as well by setting LogDir_ to a non-NFS directory. (Only
    the manager's logs will be kept permanently, the logs of
    workers/proxies are discarded upon rotation.)

*When I'm using the standalone mode, do I still need to have ``ssh`` and ``rsync`` installed and configured?*
    No. In standalone mode all operations are performed directly on the local
    file system.

*What do I need to do when something in the Bro distribution changes?*
    After pulling from the main Bro git repository, just re-run ``make
    install`` inside your build directory.  It will reinstall all the
    files from the distribution that are not up-to-date. Then do
    ``broctl install`` to make sure everything gets pushed out.

.. _development version:

*Can I change the naming scheme that BroControl uses for archived log files?*
    Yes, set MakeArchiveName_ to a
    script that outputs the desired destination file name for an
    archived log file. The default script for that task is
    ``<BroBase>/share/broctl/scripts/make-archive-name``, which you
    can use as a template for creating your own version. See
    the beginning of that script for instructions.

*Can BroControl manage a cluster of nodes over non-global IPv6 scope (e.g. link-local)?*
    Yes, set ``ZoneID`` in ``etc/broctl.cfg`` to the zone identifier
    that the BroControl node uses to identify the scope zone
    (the ``ifconfig`` command output is usually helpful, if it doesn't
    show the zone identifier appended to the address with a '%'
    character, then it may just be the interface name).  Then in
    ``etc/node.cfg``, add a ``zone_id`` key to each node section
    representing that particular node's zone identifier and set
    the ``host`` key to the IPv6 address assigned to the node within
    the scope zone.  Most nodes probably have the same ``zone_id``, but
    may not if their interface configuration differs.  See RFC 4007 for
    more information on IPv6 scoped addresses and zones.
